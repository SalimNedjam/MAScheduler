--- linux-stable/include/linux/sched.h	2020-06-08 21:09:32.617918440 +0200
+++ MAScheduler/linux-4.19.3/include/linux/sched.h	2020-06-08 13:49:33.412342501 +0200
@@ -641,6 +641,15 @@ struct task_struct {
 	int				normal_prio;
 	unsigned int			rt_priority;
 
+	/*
+	 * MAS code:
+	 */
+	struct list_head futex_state_list;
+	raw_spinlock_t futex_state_lock;
+	struct futex_state *waiting_futex_state;
+	int user_nice;
+	int futex_state_prio;
+
 	const struct sched_class	*sched_class;
 	struct sched_entity		se;
 	struct sched_rt_entity		rt;
@@ -1499,6 +1508,10 @@ static inline int set_cpus_allowed_ptr(s
 #endif
 
 extern int yield_to(struct task_struct *p, bool preempt);
+/*
+ * MAS code:
+ */
+extern void set_static_prio(struct task_struct *p);
 extern void set_user_nice(struct task_struct *p, long nice);
 extern int task_prio(const struct task_struct *p);
 

--- linux-stable/include/linux/futex.h	2020-06-08 21:09:32.484585103 +0200
+++ MAScheduler/linux-4.19.3/include/linux/futex.h	2020-06-08 13:49:33.412342501 +0200
@@ -4,6 +4,9 @@
 
 #include <linux/ktime.h>
 #include <uapi/linux/futex.h>
+#include <linux/rtmutex.h>
+
+#include <linux/kref.h>
 
 struct inode;
 struct mm_struct;
@@ -13,6 +16,52 @@ extern int
 handle_futex_death(u32 __user *uaddr, struct task_struct *curr, int pi);
 
 /*
+ * MAS code:
+ */
+extern void free_futex_state(struct kref *kref);
+extern int get_futex_state_sumload(struct task_struct *task);
+extern void futex_state_prio(struct task_struct *task);
+extern int futex_state_inherit(struct task_struct *task, 
+																struct futex_state *state,
+																int op);
+
+#define FUTEX_STATE_LOAD 			1
+#define FUTEX_STATE_UNLOAD		-1
+#define FUTEX_STATE_MAX_PRIO 	99
+
+extern int FUTEX_STATE_DEBUG;
+extern int FUTEX_STATE_ENABLE;
+
+#define debug_futex_state(fmt, ...) \
+  do { \
+		if (FUTEX_STATE_DEBUG) \
+			pr_info("(pid:%d) MAS %s: " fmt, \
+      	current->pid, __func__, ##__VA_ARGS__); \
+	} while (0)
+
+
+extern void set_prio(int tid, int prio);
+
+/**
+ * struct futex_state - The state struct to monitor futex owner
+ * @list: 	the list of the states                                                                                               :		priority-sorted list of tasks waiting on this futex
+ * @mutex: 	the lock of the state
+ * @owner: 	the the task_struct if the owner of the futex
+ * @refcount:	the kref counter
+ * @load: 	the futex load, represent the number of waiters on the futex		
+ * @key: 	the key the futex is hashed on
+ */
+struct futex_state {
+	struct list_head list;
+	struct task_struct *owner;
+	raw_spinlock_t spin_lock;
+	struct kref refcount;
+	int load;
+	union futex_key *key;
+} __randomize_layout;
+
+
+/*
  * Futexes are matched on equal values of this key.
  * The key type depends on whether it's a shared or private mapping.
  * Don't rearrange members without looking at hash_futex().

diff -uprN -X linux-stable/Documentation/dontdiff linux-stable/kernel/exit.c MAScheduler/linux-4.19.3/kernel/exit.c
--- linux-stable/kernel/exit.c	2020-06-08 21:09:32.827918446 +0200
+++ MAScheduler/linux-4.19.3/kernel/exit.c	2020-06-08 01:35:05.811417593 +0200
@@ -62,6 +62,7 @@
 #include <linux/random.h>
 #include <linux/rcuwait.h>
 #include <linux/compat.h>
+#include <linux/kref.h>
 
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
@@ -767,6 +768,25 @@ void __noreturn do_exit(long code)
 	struct task_struct *tsk = current;
 	int group_dead;
 
+	/*
+	* MAS code:
+	* if the current exiting task was waiting on a futex
+	* we have to decrement the load of the futex state
+	* and change the priority of the futex owner
+	* it's useless to delete all the futex state of the futext_state_list
+	* because memory will be released and futex state added to the new
+	* futex_state_list futex owner
+	*/
+	/* If the current exiting task was waiting on a futex */
+
+	if (FUTEX_STATE_ENABLE) {
+		if (tsk->waiting_futex_state != NULL) {
+			debug_futex_state("exit\n");
+			futex_state_inherit(tsk, tsk->waiting_futex_state, FUTEX_STATE_UNLOAD);
+			kref_put(&tsk->waiting_futex_state->refcount, free_futex_state);
+		}
+	}
+
 	profile_task_exit(tsk);
 	kcov_task_exit(tsk);
 
diff -uprN -X linux-stable/Documentation/dontdiff linux-stable/kernel/fork.c MAScheduler/linux-4.19.3/kernel/fork.c
--- linux-stable/kernel/fork.c	2020-06-08 21:09:32.827918446 +0200
+++ MAScheduler/linux-4.19.3/kernel/fork.c	2020-06-08 13:49:33.412342501 +0200
@@ -1709,6 +1709,20 @@ static __latent_entropy struct task_stru
 		goto fork_out;
 
 	/*
+	 * MAS code:
+	 */
+	/* Init the futex_state_lock */
+	raw_spin_lock_init(&p->futex_state_lock);
+	/* Init the futex_state_list */
+	INIT_LIST_HEAD(&p->futex_state_list);
+	/* Init the waiting_futex_state as NULL */
+	p->waiting_futex_state = NULL;
+	/* Init the user_nice value to 0 */
+	p->user_nice = 0;
+	/* Init the futex_state_prio value to 0 */
+	p->futex_state_prio = 0;
+
+	/*
 	 * This _must_ happen before we call free_task(), i.e. before we jump
 	 * to any of the bad_fork_* labels. This is to avoid freeing
 	 * p->set_child_tid which is (ab)used as a kthread's data pointer for
diff -uprN -X linux-stable/Documentation/dontdiff linux-stable/kernel/futex.c MAScheduler/linux-4.19.3/kernel/futex.c
--- linux-stable/kernel/futex.c	2020-06-08 21:09:32.831251778 +0200
+++ MAScheduler/linux-4.19.3/kernel/futex.c	2020-06-08 13:49:33.415675835 +0200
@@ -67,6 +67,8 @@
 #include <linux/freezer.h>
 #include <linux/bootmem.h>
 #include <linux/fault-inject.h>
+#include <linux/kref.h>
+#include <linux/shrinker.h>
 
 #include <asm/futex.h>
 
@@ -214,6 +216,17 @@ struct futex_pi_state {
 	union futex_key key;
 } __randomize_layout;
 
+
+/*
+ * MAS code:
+ */
+static struct kmem_cache *kmem_state_cache, *kmem_state_key_cache;
+/* Variable de debug pouvant être modifié avecdes modules */
+int FUTEX_STATE_DEBUG = 0;
+EXPORT_SYMBOL_GPL(FUTEX_STATE_DEBUG);
+int FUTEX_STATE_ENABLE = 1;
+EXPORT_SYMBOL_GPL(FUTEX_STATE_ENABLE);
+
 /**
  * struct futex_q - The hashed futex queue entry, one per waiting task
  * @list:		priority-sorted list of tasks waiting on this futex
@@ -778,6 +791,232 @@ static int get_futex_value_locked(u32 *d
 
 
 /*
+ * MAS code:
+ * functions to manage futex state
+ */
+
+
+/**
+ * fetch_futex_state - Search in a state associated with the key 
+ * 														is in the list
+ * @key:	Pointer to key
+ * @state_ret:	Pointer to return the stated found
+ *
+ * Put the adresse of the state if found, or NULL if not
+ */
+int fetch_futex_state(struct task_struct *task,
+											union futex_key *key, 
+											struct futex_state **state_ret)
+{
+	struct futex_state *state, *ret = NULL;
+
+	raw_spin_lock(&task->futex_state_lock);
+	list_for_each_entry(state, &task->futex_state_list, list) {
+		if (match_futex(key, state->key)) {
+			ret = state;
+			break;
+		}
+	}
+	raw_spin_unlock(&task->futex_state_lock);
+	*state_ret = ret;
+
+	return 0;
+}
+
+int get_futex_state_sumload(struct task_struct *task)
+{
+	struct futex_state *state;
+	int sumload = 0, nr = 0;
+
+	raw_spin_lock(&task->futex_state_lock);
+	list_for_each_entry(state, &task->futex_state_list, list) {
+		debug_futex_state("futex_state=%p, load=%d\n",state, state->load);
+		raw_spin_lock(&state->spin_lock);
+		sumload += state->load;
+		nr++;
+		raw_spin_unlock(&state->spin_lock);
+	}
+	debug_futex_state("task=%d have %d futex_state, sumload=%d\n", 
+		task_pid_vnr(task), nr, sumload);
+	raw_spin_unlock(&task->futex_state_lock);
+
+	return sumload;
+}
+
+int add_futex_state(struct futex_state *state)
+{
+	raw_spin_lock(&state->owner->futex_state_lock);
+	list_add(&state->list, &state->owner->futex_state_list);
+	debug_futex_state("add futex_state=%p to task=%d\n", state,
+		task_pid_vnr(state->owner));
+	raw_spin_unlock(&state->owner->futex_state_lock);
+
+	return 0;
+}
+
+int del_futex_state(struct futex_state *state)
+{
+	raw_spin_lock(&state->owner->futex_state_lock);
+	list_del(&state->list);
+	debug_futex_state("remove futex_state=%p to task=%d\n", state,
+		task_pid_vnr(state->owner));
+	raw_spin_unlock(&state->owner->futex_state_lock);
+
+	return 0;
+}
+
+/**
+ * free_futex_state - Free the space, passed as callback of kref_put
+ * @kref:	Pointer to the kref of the state
+
+ * Free the state allocated in the slab, this methode is called by kref_put
+ * when the number of references drops to zero
+ */
+void free_futex_state(struct kref *kref)
+{
+	struct futex_state *state =
+		container_of(kref, struct futex_state, refcount);
+
+	del_futex_state(state);
+	debug_futex_state("futex_state=%p\n",state);
+	put_futex_key(state->key);
+	kmem_cache_free(kmem_state_key_cache, state->key);
+	kmem_cache_free(kmem_state_cache, state);
+}
+
+/**
+ * futex_state_prio - Set the priority of the futex owner based on its load
+ * @task:	Pointer to the task
+ */
+void futex_state_prio(struct task_struct *task)
+{
+	int load;
+
+	debug_futex_state("let's change the priority of task=%d\n",
+		task_pid_vnr(task));
+
+	load = get_futex_state_sumload(task);
+
+	if (load < 0)
+		load = 0;
+
+	if (load > FUTEX_STATE_MAX_PRIO)
+		load = FUTEX_STATE_MAX_PRIO;
+
+	task->futex_state_prio = load;
+	set_static_prio(task);
+
+	debug_futex_state("task=%d, load=%d, static_prio=%d, normal_prio=%d\n",
+		task_pid_vnr(task), load, task->static_prio, task->normal_prio);
+}
+
+/**
+ * fixup_state_owner_current - Set the current task as owner of the state
+ * @state:	Pointer to the state
+ * 
+ * After set the current task as owner, put the ref
+ * In fact, the owner does not wait anymore on the futex
+ * Decrement the futex load and update the current task priority base on the 
+ * new load
+ */
+int fixup_state_owner_current(struct futex_state *state)
+{
+	int last_owner, sumload;
+	
+	debug_futex_state("let's fixup the owner of state=%p\n", state);
+
+	last_owner = task_pid_vnr(state->owner);
+	sumload = get_futex_state_sumload(current);
+
+	/* Current task became the owner */
+	state->owner = current;
+	add_futex_state(state);
+
+	/* Load decrement */
+	raw_spin_lock(&state->spin_lock);
+	state->load -= (sumload + 1);
+	raw_spin_unlock(&state->spin_lock);
+
+	/* Set the priority */
+	futex_state_prio(state->owner);
+
+
+	/* Current task no longer wait on the futex */
+	kref_put(&state->refcount, free_futex_state);
+	current->waiting_futex_state = NULL;
+
+	debug_futex_state("futex_state=%p, ref=%d, last_owner=%d, new_owner=%d\n",
+		state, kref_read(&state->refcount), last_owner, task_pid_vnr(state->owner));
+
+	return 0;
+}
+
+int futex_state_inherit(struct task_struct *task, 
+												struct futex_state *state,
+												int op)
+{
+	int sumload = 0;
+	struct futex_state *m_state;
+
+	/* Is valid operation */
+	if (op != FUTEX_STATE_LOAD && op != FUTEX_STATE_UNLOAD)
+		return -1;
+
+	debug_futex_state("apply inherit on futex_state=%p, op=%d\n", state, op);
+
+	/* Get the sum of all the futex state load on the task */
+	sumload = get_futex_state_sumload(task);
+
+	/*
+	 * Apply the load inheritance 
+	 * increment the load of the task futex state
+	 * if the futex owner is waiting on a futex, then increment
+	 * his load too, and repeat until the futex owner does not
+	 * wait on a futex, wich will be considered as the master futex owner
+	 */
+	do {
+		m_state = state;
+		raw_spin_lock(&state->spin_lock);
+		state->load += (sumload + 1) * op;
+		raw_spin_unlock(&state->spin_lock);
+		debug_futex_state("do inherit, futex_state=%p, load=%d, owner=%d\n",
+			state, state->load, task_pid_vnr(state->owner));
+	}	while ((state = state->owner->waiting_futex_state) != NULL);
+	
+	futex_state_prio(m_state->owner);
+
+	return 0;
+}
+
+static int get_futex_state(struct task_struct *owner,
+													 union futex_key *key,
+													 struct futex_state **state_ret)
+{
+	/* Check if the key match a state or it's the first task to wait */
+	fetch_futex_state(owner, key, state_ret);
+	if (!*state_ret) {
+		/* It's the first task to wait so we create the state */
+		*state_ret = kmem_cache_zalloc(kmem_state_cache, GFP_KERNEL);
+		(*state_ret)->key = key;
+		(*state_ret)->owner = owner;
+		(*state_ret)->load = 0;
+		kref_init(&(*state_ret)->refcount);
+		debug_futex_state("create futex_state=%p\n", *state_ret);
+		raw_spin_lock_init(&(*state_ret)->spin_lock);
+		add_futex_state(*state_ret);
+	} else {
+		debug_futex_state("exists futex_state=%p\n", *state_ret);
+		/* Dealloc the state key */
+		put_futex_key(key);
+		kmem_cache_free(kmem_state_key_cache, key);
+		kref_get(&(*state_ret)->refcount);
+	}
+
+	return 0;
+}
+
+
+/*
  * PI code:
  */
 static int refill_pi_state_cache(void)
@@ -2716,7 +2955,21 @@ static int futex_lock_pi(u32 __user *uad
 	struct rt_mutex_waiter rt_waiter;
 	struct futex_hash_bucket *hb;
 	struct futex_q q = futex_q_init;
+	struct task_struct *owner;
+	struct futex_state *state = NULL;
+	union futex_key *key;
 	int res, ret;
+	u32 uval, vpid;
+
+	/* Alloc memory for the state key */
+	key = kmem_cache_zalloc(kmem_state_key_cache, GFP_KERNEL);
+	/* Get the user value of the lock */
+	if (get_user(uval, uaddr))
+		return -EFAULT;
+	/* Get the owner pid of the lock  */
+	vpid = uval & FUTEX_TID_MASK;
+	/* Get the owner task_struct */
+	owner = find_task_by_vpid(vpid);
 
 	if (!IS_ENABLED(CONFIG_FUTEX_PI))
 		return -ENOSYS;
@@ -2724,6 +2977,8 @@ static int futex_lock_pi(u32 __user *uad
 	if (refill_pi_state_cache())
 		return -ENOMEM;
 
+
+
 	if (time) {
 		to = &timeout;
 		hrtimer_init_on_stack(&to->timer, CLOCK_REALTIME,
@@ -2735,8 +2990,34 @@ static int futex_lock_pi(u32 __user *uad
 retry:
 	ret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q.key, VERIFY_WRITE);
 	if (unlikely(ret != 0))
-		goto out;
+		goto out_dealloc_key;
 
+	/*
+	 * MAS code:
+	 * get or create the futex state for the futex owner
+	 * calcul his load and set the priority based on
+	 */ 
+	if (FUTEX_STATE_ENABLE) {
+		/* 
+		* For security we do not use the futex_q.key 'q.key'
+		* by using our own key we can manage his deallocation
+		*/
+		ret = get_futex_key(uaddr, 0, key, VERIFY_READ);
+		if (unlikely(ret != 0))
+			goto out_dealloc_put_key;
+
+		/* Get the futex state associed to the key, if not exists creating one */
+		get_futex_state(owner, key, &state);
+		/* Current task will be waiting on the futex state */
+		current->waiting_futex_state = state;
+		/* Apply the load inherit */
+		futex_state_inherit(current, state, FUTEX_STATE_LOAD);
+
+		debug_futex_state("current task will wait on futex_state=%p, load=%d, \
+owner=%d, owner->normal_prio=%d\n", state, state->load,
+task_pid_vnr(state->owner), state->owner->normal_prio);
+	}
+	
 retry_private:
 	hb = queue_lock(&q);
 
@@ -2750,6 +3031,11 @@ retry_private:
 		case 1:
 			/* We got the lock. */
 			ret = 0;
+			/* Put this task as owner of futex state */
+			if (FUTEX_STATE_ENABLE) {
+				debug_futex_state("got the lock without waiting\n");
+				fixup_state_owner_current(state);
+			}
 			goto out_unlock_put_key;
 		case -EFAULT:
 			goto uaddr_faulted;
@@ -2835,6 +3121,11 @@ no_block:
 	 * haven't already.
 	 */
 	res = fixup_owner(uaddr, &q, !ret);
+
+	/* Put this task as owner of futex state */
+	if (FUTEX_STATE_ENABLE)
+		fixup_state_owner_current(state);
+
 	/*
 	 * If fixup_owner() returned an error, proprogate that.  If it acquired
 	 * the lock, clear our -ETIMEDOUT or -EINTR.
@@ -2873,6 +3164,16 @@ out:
 	}
 	return ret != -EINTR ? ret : -ERESTARTNOINTR;
 
+out_dealloc_key:
+	/* Dealloc the state key */
+	kmem_cache_free(kmem_state_key_cache, key);
+	goto out;
+
+out_dealloc_put_key:
+	/* Dealloc the state key */
+	kmem_cache_free(kmem_state_key_cache, key);
+	goto out_put_key;
+
 uaddr_faulted:
 	queue_unlock(hb);
 
@@ -2884,6 +3185,7 @@ uaddr_faulted:
 		goto retry_private;
 
 	put_futex_key(&q.key);
+	put_futex_key(key);
 	goto retry;
 }
 
@@ -2898,6 +3200,7 @@ static int futex_unlock_pi(u32 __user *u
 	union futex_key key = FUTEX_KEY_INIT;
 	struct futex_hash_bucket *hb;
 	struct futex_q *top_waiter;
+	struct futex_state *state;
 	int ret;
 
 	if (!IS_ENABLED(CONFIG_FUTEX_PI))
@@ -2909,6 +3212,7 @@ retry:
 	/*
 	 * We release only a lock we actually own:
 	 */
+	
 	if ((uval & FUTEX_TID_MASK) != vpid)
 		return -EPERM;
 
@@ -2920,6 +3224,22 @@ retry:
 	spin_lock(&hb->lock);
 
 	/*
+	 * MAS code:
+	 * release the futex state if exists
+	 */ 
+	if (FUTEX_STATE_ENABLE) {
+		fetch_futex_state(current, &key, &state);
+		if (state) {
+			del_futex_state(state);
+			debug_futex_state("futex_state=%p\n", state);
+			/* When release a futex the load change, set the new priority */
+			futex_state_prio(current);
+		} else {
+			debug_futex_state("no futex state to unlock\n");
+		}
+	}
+
+	/*
 	 * Check waiters first. We do not trust user space values at
 	 * all and we at least want to know if user space fiddled
 	 * with the futex value instead of blindly unlocking.
@@ -3635,6 +3955,17 @@ static int __init futex_init(void)
 		spin_lock_init(&futex_queues[i].lock);
 	}
 
+	/*
+	 * MAS code:
+	 * init slab
+	 */
+	kmem_state_cache = KMEM_CACHE(futex_state, 0);
+	kmem_state_key_cache = kmem_cache_create("futex_key",
+			sizeof(union futex_key),
+			__alignof__(union futex_key),
+			0,
+			NULL);
+
 	return 0;
 }
 core_initcall(futex_init);
diff -uprN -X linux-stable/Documentation/dontdiff linux-stable/kernel/sched/core.c MAScheduler/linux-4.19.3/kernel/sched/core.c
--- linux-stable/kernel/sched/core.c	2020-06-08 21:09:32.861251779 +0200
+++ MAScheduler/linux-4.19.3/kernel/sched/core.c	2020-06-08 13:49:33.419009168 +0200
@@ -3863,15 +3863,23 @@ static inline int rt_effective_prio(stru
 }
 #endif
 
-void set_user_nice(struct task_struct *p, long nice)
+/*
+ * MAS code:
+ * Applique la priorité d'une tâche en prenant en compte
+ * le nice utilisateur et la priorité du futex_state
+ */
+void set_static_prio(struct task_struct *p)
 {
 	bool queued, running;
-	int old_prio, delta;
+	int old_prio, delta, prio;
 	struct rq_flags rf;
 	struct rq *rq;
+	
+	prio = NICE_TO_PRIO(p->user_nice) - p->futex_state_prio;
+
+	if (prio < 0)
+		prio = 0;
 
-	if (task_nice(p) == nice || nice < MIN_NICE || nice > MAX_NICE)
-		return;
 	/*
 	 * We have to be careful, if called from sys_setpriority(),
 	 * the task might be in the middle of scheduling on another CPU.
@@ -3886,7 +3894,7 @@ void set_user_nice(struct task_struct *p
 	 * SCHED_DEADLINE, SCHED_FIFO or SCHED_RR:
 	 */
 	if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
-		p->static_prio = NICE_TO_PRIO(nice);
+		p->static_prio = prio;
 		goto out_unlock;
 	}
 	queued = task_on_rq_queued(p);
@@ -3896,7 +3904,7 @@ void set_user_nice(struct task_struct *p
 	if (running)
 		put_prev_task(rq, p);
 
-	p->static_prio = NICE_TO_PRIO(nice);
+	p->static_prio = prio;
 	set_load_weight(p, true);
 	old_prio = p->prio;
 	p->prio = effective_prio(p);
@@ -3916,6 +3924,15 @@ void set_user_nice(struct task_struct *p
 out_unlock:
 	task_rq_unlock(rq, p, &rf);
 }
+
+void set_user_nice(struct task_struct *p, long nice)
+{
+	if (task_nice(p) == nice || nice < MIN_NICE || nice > MAX_NICE)
+		return;
+	
+	p->user_nice = nice;
+	set_static_prio(p);
+}
 EXPORT_SYMBOL(set_user_nice);
 
 /*
diff -uprN -X linux-stable/Documentation/dontdiff linux-stable/kernel/sched/fair.c MAScheduler/linux-4.19.3/kernel/sched/fair.c
--- linux-stable/kernel/sched/fair.c	2020-06-08 21:09:32.867918446 +0200
+++ MAScheduler/linux-4.19.3/kernel/sched/fair.c	2020-06-08 21:43:43.944634946 +0200
@@ -802,6 +802,7 @@ static void update_tg_load_avg(struct cf
 static void update_curr(struct cfs_rq *cfs_rq)
 {
 	struct sched_entity *curr = cfs_rq->curr;
+
 	u64 now = rq_clock_task(rq_of(cfs_rq));
 	u64 delta_exec;
 
Binary files linux-stable/scripts/asn1_compiler and MAScheduler/linux-4.19.3/scripts/asn1_compiler differ
Binary files linux-stable/scripts/extract-cert and MAScheduler/linux-4.19.3/scripts/extract-cert differ
Binary files linux-stable/scripts/selinux/mdp/mdp and MAScheduler/linux-4.19.3/scripts/selinux/mdp/mdp differ
diff -uprN -X linux-stable/Documentation/dontdiff linux-stable/std3z6eg MAScheduler/linux-4.19.3/std3z6eg
--- linux-stable/std3z6eg	1970-01-01 01:00:00.000000000 +0100
+++ MAScheduler/linux-4.19.3/std3z6eg	2020-06-08 01:35:05.814750926 +0200
@@ -0,0 +1 @@
+!<thin>
Binary files linux-stable/tools/objtool/objtool and MAScheduler/linux-4.19.3/tools/objtool/objtool differ
